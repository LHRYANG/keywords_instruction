# analyze_keywords.py

import json
from pathlib import Path
from collections import defaultdict
from nltk.stem import PorterStemmer
from typing import List, Dict, Tuple, Set

ps = PorterStemmer()

def normalize_word(word: str) -> str:
    """Normalize word to its stem."""
    return ps.stem(word.lower().strip())

# ---------------------------
# ğŸ“¦ åˆ†æç±»å‹1ï¼šå…³é”®è¯é¢‘ç‡ç»“æ„
# ---------------------------
def merge_freq_jsons(freq_json_paths: List[str], threshold: int = 10) -> Tuple[Dict[str, Dict[str, int]], Dict[str, int]]:
    """
    åˆå¹¶ keyword_length æ ¼å¼çš„ JSON æ–‡ä»¶ï¼Œå¹¶ç»Ÿè®¡æ¯ä¸ªé•¿åº¦ä¸‹é¢‘ç‡å¤§äºé˜ˆå€¼çš„è¯æ•°é‡ã€‚
    
    è¿”å›ï¼š
        merged: åˆå¹¶åçš„è¯é¢‘å­—å…¸
        count_result: æ¯ä¸ªé•¿åº¦ä¸‹çš„å…³é”®è¯è®¡æ•°ï¼ˆfreq > thresholdï¼‰
    """
    merged = defaultdict(lambda: defaultdict(int))

    for path in freq_json_paths:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        for length_key, kw_dict in data.items():
            for word, freq in kw_dict.items():
                merged[length_key][word] += freq

    count_result = {}
    for length_key, kw_dict in merged.items():
        count = sum(1 for freq in kw_dict.values() if freq > threshold)
        count_result[length_key] = count

    return dict(merged), count_result


# ---------------------------
# ğŸ“¦ åˆ†æç±»å‹2ï¼šè¯­ä¹‰åˆ†ç±»å…³é”®è¯ç»“æ„
# ---------------------------
def merge_class_jsons(class_json_paths: List[str]) -> Set[str]:
    """ä»ç±»åˆ«å…³é”®è¯ JSON ä¸­æå–æ‰€æœ‰å…³é”®è¯ï¼Œä¸å»é‡ã€ä¸è¯é¢‘ï¼Œä»…èšåˆ"""
    all_keywords = set()
    for path in class_json_paths:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        for word_list in data.values():
            all_keywords.update(word_list)
    return all_keywords


# ---------------------------
# ğŸ”„ åˆå¹¶é¢‘ç‡ç±» + åˆ†ç±»ç±»å…³é”®è¯ï¼Œå¹¶æŒ‰è¯å¹²å»é‡
# ---------------------------
def merge_and_stem_keywords(
    freq_json_paths: List[str] = None,
    class_json_paths: List[str] = None,
    threshold: int = 10
) -> Set[str]:
    freq_json_paths = freq_json_paths or []
    class_json_paths = class_json_paths or []

    all_keywords = set()

    # åŠ è½½é¢‘ç‡ç±» JSON
    for path in freq_json_paths:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        for kw_dict in data.values():
            for word, freq in kw_dict.items():
                if freq > threshold:
                    all_keywords.add(word)

    # åŠ è½½åˆ†ç±»ç±» JSON
    for path in class_json_paths:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        for word_list in data.values():
            all_keywords.update(word_list)

    # è¯å¹²åŒ– + å»é‡
    stemmed = {normalize_word(w) for w in all_keywords}
    return stemmed


# ---------------------------
# âœ… ç¤ºä¾‹è°ƒç”¨
# ---------------------------
if __name__ == "__main__":
    # æ·»åŠ æ‰€æœ‰inside_keywordsä¸­çš„JSONæ–‡ä»¶åˆ°freq_files
    freq_files = [
        "inside_keywords/edit_type_action_change_change_by_length.json",
        "inside_keywords/edit_type_add_noun_freq_by_length.json",
        "inside_keywords/edit_type_appearance_alter_change_by_length.json",
        "inside_keywords/edit_type_attribute_modification_change_by_length.json",
        "inside_keywords/edit_type_background_change_noun_freq_by_length.json",
        "inside_keywords/edit_type_color_alter_change_by_length.json",
        "inside_keywords/edit_type_env_noun_freq_by_length.json",
        "inside_keywords/edit_type_remove_noun_freq_by_length.json",
        "inside_keywords/edit_type_replace_change_by_length.json",
        "inside_keywords/edit_type_style_noun_freq_by_length.json",
        "inside_keywords/edit_type_swap_change_by_length.json",
        "inside_keywords/edit_type_tone_transfer_change_by_length.json",
        "inside_keywords/edit_type_tune_transfer_change_by_length.json"
    ]
    
    # æ·»åŠ æ‰€æœ‰outside_keywordsä¸­çš„JSONæ–‡ä»¶åˆ°class_files
    class_files = [
        "outside_keywords/keyword_action.json",
        "outside_keywords/keyword_appearance.json",
        "outside_keywords/keyword_brand.json",
        "outside_keywords/keyword_color.json",
        "outside_keywords/keyword_function_purpose.json",
        "outside_keywords/keyword_material.json",
        "outside_keywords/keyword_size_shape.json",
        "outside_keywords/keyword_spatial_position.json",
        "outside_keywords/keyword_temporal_attributes.json"
    ]

    # âœ… 1. åˆ†æé¢‘ç‡ç±»å…³é”®è¯æ–‡ä»¶ (inside_keywords)
    print("\n" + "="*60)
    print("ğŸ“Š åˆ†æé¢‘ç‡ç±»å…³é”®è¯æ–‡ä»¶ (inside_keywords)")
    print("="*60)
    
    total_freq_keywords = 0
    for i, freq_file in enumerate(freq_files, 1):
        print(f"\n{i}. åˆ†ææ–‡ä»¶: {freq_file}")
        try:
            with open(freq_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            file_total = 0
            length_stats = {}
            
            for length_key, kw_dict in data.items():
                high_freq_count = sum(1 for freq in kw_dict.values() if freq > 10)
                total_words = len(kw_dict)
                length_stats[length_key] = {
                    'total': total_words,
                    'high_freq': high_freq_count
                }
                file_total += total_words
            
            print(f"   ğŸ“ˆ æ€»è¯æ•°: {file_total}")
            print(f"   ğŸ“Š æŒ‰é•¿åº¦ç»Ÿè®¡:")
            for length_key, stats in length_stats.items():
                print(f"      {length_key}: {stats['total']} è¯ (é¢‘ç‡>10: {stats['high_freq']})")
            
            total_freq_keywords += file_total
            
        except Exception as e:
            print(f"   âŒ é”™è¯¯: {e}")
    
    print(f"\nğŸ“Š é¢‘ç‡ç±»æ–‡ä»¶æ€»è®¡: {total_freq_keywords} ä¸ªå…³é”®è¯")
    
    # âœ… 2. åˆ†æè¯­ä¹‰åˆ†ç±»å…³é”®è¯æ–‡ä»¶ (outside_keywords)
    print("\n" + "="*60)
    print("ğŸ·ï¸  åˆ†æè¯­ä¹‰åˆ†ç±»å…³é”®è¯æ–‡ä»¶ (outside_keywords)")
    print("="*60)
    
    total_class_keywords = 0
    for i, class_file in enumerate(class_files, 1):
        print(f"\n{i}. åˆ†ææ–‡ä»¶: {class_file}")
        try:
            with open(class_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            file_total = 0
            category_stats = {}
            
            for category, word_list in data.items():
                if isinstance(word_list, list):
                    category_stats[category] = len(word_list)
                    file_total += len(word_list)
                else:
                    category_stats[category] = 1
                    file_total += 1
            
            print(f"   ğŸ“ˆ æ€»è¯æ•°: {file_total}")
            print(f"   ğŸ“Š æŒ‰ç±»åˆ«ç»Ÿè®¡:")
            for category, count in category_stats.items():
                print(f"      {category}: {count} è¯")
            
            total_class_keywords += file_total
            
        except Exception as e:
            print(f"   âŒ é”™è¯¯: {e}")
    
    print(f"\nğŸ·ï¸  åˆ†ç±»æ–‡ä»¶æ€»è®¡: {total_class_keywords} ä¸ªå…³é”®è¯")
    
    # # âœ… 3. åˆå¹¶åˆ†æ
    # print("\n" + "="*60)
    # print("ğŸ”— åˆå¹¶åˆ†æ")
    # print("="*60)
    
    # merged_freq, count_result = merge_freq_jsons(freq_files)
    # print("\nğŸ“Š åˆå¹¶åçš„é¢‘ç‡ç»Ÿè®¡ (é¢‘ç‡ > 10):")
    # total_high_freq = 0
    # for k, v in sorted(count_result.items()):
    #     print(f"   {k}: {v} è¯")
    #     total_high_freq += v
    # print(f"   æ€»è®¡é«˜é¢‘è¯: {total_high_freq}")

    # class_keywords = merge_class_jsons(class_files)
    # print(f"\nğŸ·ï¸  åˆå¹¶åçš„åˆ†ç±»å…³é”®è¯: {len(class_keywords)} ä¸ª")

    # stemmed_keywords = merge_and_stem_keywords(freq_files, class_files)
    # print(f"\nğŸŒ± è¯å¹²åŒ–åçš„å”¯ä¸€å…³é”®è¯: {len(stemmed_keywords)} ä¸ª")
    # print("ğŸ” æ ·æœ¬:", list(stemmed_keywords)[:20])
